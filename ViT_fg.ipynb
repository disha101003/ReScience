{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdQfX5XDhEKV"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncUOD_akmSF2"
      },
      "outputs": [],
      "source": [
        "class NewGELUActivation(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT). Also see\n",
        "    the Gaussian Error Linear Units paper: https://arxiv.org/abs/1606.08415\n",
        "\n",
        "    Taken from https://github.com/huggingface/transformers/blob/main/src/transformers/activations.py\n",
        "    \"\"\"\n",
        "\n",
        "    def forward(self, input):\n",
        "        return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
        "\n",
        "\n",
        "class PatchEmbeddings(nn.Module):\n",
        "    \"\"\"\n",
        "    Convert the image into patches and then project them into a vector space.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.image_size = config[\"image_size\"]\n",
        "        self.patch_size = config[\"patch_size\"]\n",
        "        self.num_channels = config[\"num_channels\"]\n",
        "        self.hidden_size = config[\"hidden_size\"]\n",
        "        # Calculate the number of patches from the image size and patch size\n",
        "        self.num_patches = (self.image_size // self.patch_size) ** 2\n",
        "        # Create a projection layer to convert the image into patches\n",
        "        # The layer projects each patch into a vector of size hidden_size\n",
        "        self.projection = nn.Conv2d(self.num_channels, self.hidden_size, kernel_size=self.patch_size, stride=self.patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (batch_size, num_channels, image_size, image_size) -> (batch_size, num_patches, hidden_size)\n",
        "        x = self.projection(x)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Embeddings(nn.Module):\n",
        "    \"\"\"\n",
        "    Combine the patch embeddings with the class token and position embeddings.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.patch_embeddings = PatchEmbeddings(config)\n",
        "        # Create a learnable [CLS] token\n",
        "        # Similar to BERT, the [CLS] token is added to the beginning of the input sequence\n",
        "        # and is used to classify the entire sequence\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, config[\"hidden_size\"]))\n",
        "        # Create position embeddings for the [CLS] token and the patch embeddings\n",
        "        # Add 1 to the sequence length for the [CLS] token\n",
        "        self.position_embeddings = \\\n",
        "            nn.Parameter(torch.randn(1, self.patch_embeddings.num_patches + 1, config[\"hidden_size\"]))\n",
        "        self.dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_embeddings(x)\n",
        "        batch_size, _, _ = x.size()\n",
        "        # Expand the [CLS] token to the batch size\n",
        "        # (1, 1, hidden_size) -> (batch_size, 1, hidden_size)\n",
        "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
        "        # Concatenate the [CLS] token to the beginning of the input sequence\n",
        "        # This results in a sequence length of (num_patches + 1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = x + self.position_embeddings\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class AttentionHead(nn.Module):\n",
        "    \"\"\"\n",
        "    A single attention head.\n",
        "    This module is used in the MultiHeadAttention module.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size, attention_head_size, dropout, bias=True):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.attention_head_size = attention_head_size\n",
        "        # Create the query, key, and value projection layers\n",
        "        self.query = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
        "        self.key = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
        "        self.value = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x,c):\n",
        "        # Project the input into query, key, and value\n",
        "        # The same input is used to generate the query, key, and value,\n",
        "        # so it's usually called self-attention.\n",
        "        # (batch_size, sequence_length, hidden_size) -> (batch_size, sequence_length, attention_head_size)\n",
        "        query = self.query(x)\n",
        "        key = self.key(x)\n",
        "        value = self.value(x)\n",
        "        # Calculate the attention scores\n",
        "        # softmax(Q*K.T/sqrt(head_size))*V\n",
        "        attention_scores = torch.matmul(query, key.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
        "        if(c==1):\n",
        "            attention_probs = attention_probs + torch.randn(attention_probs.size()).to(device)*std + mean\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "        # Calculate the attention output\n",
        "        attention_output = torch.matmul(attention_probs, value)\n",
        "        return (attention_output, attention_probs)\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-head attention module.\n",
        "    This module is used in the TransformerEncoder module.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.hidden_size = config[\"hidden_size\"]\n",
        "        self.num_attention_heads = config[\"num_attention_heads\"]\n",
        "        # The attention head size is the hidden size divided by the number of attention heads\n",
        "        self.attention_head_size = self.hidden_size // self.num_attention_heads\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "        # Whether or not to use bias in the query, key, and value projection layers\n",
        "        self.qkv_bias = config[\"qkv_bias\"]\n",
        "        # Create a list of attention heads\n",
        "        self.heads = nn.ModuleList([])\n",
        "        for _ in range(self.num_attention_heads):\n",
        "            head = AttentionHead(\n",
        "                self.hidden_size,\n",
        "                self.attention_head_size,\n",
        "                config[\"attention_probs_dropout_prob\"],\n",
        "                self.qkv_bias\n",
        "            )\n",
        "            self.heads.append(head)\n",
        "        # Create a linear layer to project the attention output back to the hidden size\n",
        "        # In most cases, all_head_size and hidden_size are the same\n",
        "        self.output_projection = nn.Linear(self.all_head_size, self.hidden_size)\n",
        "        self.output_dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
        "\n",
        "    def forward(self, x, c,output_attentions=False):\n",
        "        # Calculate the attention output for each attention head\n",
        "        attention_outputs = [head(x,c) for head in self.heads]\n",
        "        # Concatenate the attention outputs from each attention head\n",
        "        attention_output = torch.cat([attention_output for attention_output, _ in attention_outputs], dim=-1)\n",
        "        # Project the concatenated attention output back to the hidden size\n",
        "        attention_output = self.output_projection(attention_output)\n",
        "        attention_output = self.output_dropout(attention_output)\n",
        "        # Return the attention output and the attention probabilities (optional)\n",
        "        if not output_attentions:\n",
        "            return (attention_output, None)\n",
        "        else:\n",
        "            attention_probs = torch.stack([attention_probs for _, attention_probs in attention_outputs], dim=1)\n",
        "            return (attention_output, attention_probs)\n",
        "\n",
        "\n",
        "class FasterMultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-head attention module with some optimizations.\n",
        "    All the heads are processed simultaneously with merged query, key, and value projections.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.hidden_size = config[\"hidden_size\"]\n",
        "        self.num_attention_heads = config[\"num_attention_heads\"]\n",
        "        # The attention head size is the hidden size divided by the number of attention heads\n",
        "        self.attention_head_size = self.hidden_size // self.num_attention_heads\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "        # Whether or not to use bias in the query, key, and value projection layers\n",
        "        self.qkv_bias = config[\"qkv_bias\"]\n",
        "        # Create a linear layer to project the query, key, and value\n",
        "        self.qkv_projection = nn.Linear(self.hidden_size, self.all_head_size * 3, bias=self.qkv_bias)\n",
        "        self.attn_dropout = nn.Dropout(config[\"attention_probs_dropout_prob\"])\n",
        "        # Create a linear layer to project the attention output back to the hidden size\n",
        "        # In most cases, all_head_size and hidden_size are the same\n",
        "        self.output_projection = nn.Linear(self.all_head_size, self.hidden_size)\n",
        "        self.output_dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
        "\n",
        "    def forward(self, x,c, output_attentions=False):\n",
        "        # Project the query, key, and value\n",
        "        # (batch_size, sequence_length, hidden_size) -> (batch_size, sequence_length, all_head_size * 3)\n",
        "        qkv = self.qkv_projection(x)\n",
        "        # Split the projected query, key, and value into query, key, and value\n",
        "        # (batch_size, sequence_length, all_head_size * 3) -> (batch_size, sequence_length, all_head_size)\n",
        "        query, key, value = torch.chunk(qkv, 3, dim=-1)\n",
        "        # Resize the query, key, and value to (batch_size, num_attention_heads, sequence_length, attention_head_size)\n",
        "        batch_size, sequence_length, _ = query.size()\n",
        "        query = query.view(batch_size, sequence_length, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n",
        "        key = key.view(batch_size, sequence_length, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n",
        "        value = value.view(batch_size, sequence_length, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n",
        "        # Calculate the attention scores\n",
        "        # softmax(Q*K.T/sqrt(head_size))*V\n",
        "        attention_scores = torch.matmul(query, key.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
        "        if(c==1):\n",
        "            attention_probs = attention_probs + torch.randn(attention_probs.size()).to(device)*std + mean\n",
        "        attention_probs = self.attn_dropout(attention_probs)\n",
        "        # Calculate the attention output\n",
        "        attention_output = torch.matmul(attention_probs, value)\n",
        "        # Resize the attention output\n",
        "        # from (batch_size, num_attention_heads, sequence_length, attention_head_size)\n",
        "        # To (batch_size, sequence_length, all_head_size)\n",
        "        attention_output = attention_output.transpose(1, 2) \\\n",
        "                                           .contiguous() \\\n",
        "                                           .view(batch_size, sequence_length, self.all_head_size)\n",
        "        # Project the attention output back to the hidden size\n",
        "        attention_output = self.output_projection(attention_output)\n",
        "        attention_output = self.output_dropout(attention_output)\n",
        "        # Return the attention output and the attention probabilities (optional)\n",
        "        if not output_attentions:\n",
        "            return (attention_output, None)\n",
        "        else:\n",
        "            return (attention_output, attention_probs)\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"\"\"\n",
        "    A multi-layer perceptron module.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense_1 = nn.Linear(config[\"hidden_size\"], config[\"intermediate_size\"])\n",
        "        self.activation = NewGELUActivation()\n",
        "        self.dense_2 = nn.Linear(config[\"intermediate_size\"], config[\"hidden_size\"])\n",
        "        self.dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dense_1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.dense_2(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\"\n",
        "    A single transformer block.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.use_faster_attention = config.get(\"use_faster_attention\", False)\n",
        "        if self.use_faster_attention:\n",
        "            self.attention = FasterMultiHeadAttention(config)\n",
        "        else:\n",
        "            self.attention = MultiHeadAttention(config)\n",
        "        self.layernorm_1 = nn.LayerNorm(config[\"hidden_size\"])\n",
        "        self.mlp = MLP(config)\n",
        "        self.layernorm_2 = nn.LayerNorm(config[\"hidden_size\"])\n",
        "\n",
        "    def forward(self, x, c,output_attentions=False):\n",
        "        # Self-attention\n",
        "        attention_output, attention_probs = \\\n",
        "            self.attention(self.layernorm_1(x),c, output_attentions=output_attentions)\n",
        "        # Skip connection\n",
        "        x = x + attention_output\n",
        "        # Feed-forward network\n",
        "        mlp_output = self.mlp(self.layernorm_2(x))\n",
        "        # Skip connection\n",
        "        x = x + mlp_output\n",
        "        # Return the transformer block's output and the attention probabilities (optional)\n",
        "        if not output_attentions:\n",
        "            return (x, None)\n",
        "        else:\n",
        "            return (x, attention_probs)\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    The transformer encoder module.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,config, num_encoders):\n",
        "        super().__init__()\n",
        "        # Create a list of transformer blocks\n",
        "        self.blocks = nn.ModuleList([])\n",
        "        for _ in range(num_encoders):\n",
        "            block = Block(config)\n",
        "            self.blocks.append(block)\n",
        "\n",
        "    def forward(self, x,c, output_attentions=False):\n",
        "        # Calculate the transformer block's output for each block\n",
        "        all_attentions = []\n",
        "        for block in self.blocks:\n",
        "            x, attention_probs = block(x, c,output_attentions=output_attentions)\n",
        "            if output_attentions:\n",
        "                all_attentions.append(attention_probs)\n",
        "        # Return the encoder's output and the attention probabilities (optional)\n",
        "        if not output_attentions:\n",
        "            return (x, None)\n",
        "        else:\n",
        "            return (x, all_attentions)\n",
        "\n",
        "\n",
        "class ViTForClassfication(nn.Module):\n",
        "    \"\"\"\n",
        "    The ViT model for classification.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.image_size = config[\"image_size\"]\n",
        "        self.hidden_size = config[\"hidden_size\"]\n",
        "        self.num_classes = config[\"num_classes\"]\n",
        "        self.num_encoders = config[\"num_hidden_layers\"]\n",
        "        # Create the embedding module\n",
        "        self.embedding = Embeddings(config)\n",
        "        # Create the transformer encoder module\n",
        "        self.encoder = Encoder(config, self.num_encoders)\n",
        "        # Create a linear layer to project the encoder's output to the number of classes\n",
        "        self.classifier = nn.Linear(self.hidden_size, self.num_classes)\n",
        "        # Initialize the weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def forward(self, x,c, output_attentions=False):\n",
        "        # Calculate the embedding output\n",
        "        embedding_output = self.embedding(x)\n",
        "        # Calculate the encoder's output\n",
        "        encoder_output, all_attentions = self.encoder(embedding_output,c, output_attentions=output_attentions)\n",
        "        # Calculate the logits, take the [CLS] token's output as features for classification\n",
        "        logits = self.classifier(encoder_output[:, 0, :])\n",
        "        # Return the logits and the attention probabilities (optional)\n",
        "        if not output_attentions:\n",
        "            return (logits, None)\n",
        "        else:\n",
        "            return (logits, all_attentions)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=self.config[\"initializer_range\"])\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "        elif isinstance(module, Embeddings):\n",
        "            module.position_embeddings.data = nn.init.trunc_normal_(\n",
        "                module.position_embeddings.data.to(torch.float32),\n",
        "                mean=0.0,\n",
        "                std=self.config[\"initializer_range\"],\n",
        "            ).to(module.position_embeddings.dtype)\n",
        "\n",
        "            module.cls_token.data = nn.init.trunc_normal_(\n",
        "                module.cls_token.data.to(torch.float32),\n",
        "                mean=0.0,\n",
        "                std=self.config[\"initializer_range\"],\n",
        "            ).to(module.cls_token.dtype)\n",
        "\n",
        "class ViTForClassfication_g(nn.Module):\n",
        "    \"\"\"\n",
        "    The ViT model for classification.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.image_size = config[\"image_size\"]\n",
        "        self.hidden_size = config[\"hidden_size\"]\n",
        "        self.num_classes = config[\"num_classes\"]\n",
        "        # Create the embedding module\n",
        "        self.embedding = Embeddings(config)\n",
        "        # Create the transformer encoder module\n",
        "        self.num_encoders = 2\n",
        "        self.encoder = Encoder(config, self.num_encoders)\n",
        "        # Create a linear layer to project the encoder's output to the number of classes\n",
        "        self.classifier = nn.Linear(self.hidden_size, self.num_classes)\n",
        "        # Initialize the weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def forward(self, x,c, output_attentions=False):\n",
        "        # Calculate the embedding output\n",
        "        embedding_output = self.embedding(x)\n",
        "        # Calculate the encoder's output\n",
        "        encoder_output, all_attentions = self.encoder(embedding_output,c, output_attentions=output_attentions)\n",
        "\n",
        "        if not output_attentions:\n",
        "            return (encoder_output, None)\n",
        "        else:\n",
        "            return (encoder_output, all_attentions)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=self.config[\"initializer_range\"])\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "        elif isinstance(module, Embeddings):\n",
        "            module.position_embeddings.data = nn.init.trunc_normal_(\n",
        "                module.position_embeddings.data.to(torch.float32),\n",
        "                mean=0.0,\n",
        "                std=self.config[\"initializer_range\"],\n",
        "            ).to(module.position_embeddings.dtype)\n",
        "\n",
        "            module.cls_token.data = nn.init.trunc_normal_(\n",
        "                module.cls_token.data.to(torch.float32),\n",
        "                mean=0.0,\n",
        "                std=self.config[\"initializer_range\"],\n",
        "            ).to(module.cls_token.dtype)\n",
        "\n",
        "class ViTForClassfication_f(nn.Module):\n",
        "    \"\"\"\n",
        "    The ViT model for classification.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.image_size = config[\"image_size\"]\n",
        "        self.hidden_size = config[\"hidden_size\"]\n",
        "        self.num_classes = config[\"num_classes\"]\n",
        "        # Create the embedding module\n",
        "        self.embedding = Embeddings(config)\n",
        "        # Create the transformer encoder module\n",
        "        self.num_encoders = 2\n",
        "        self.encoder = Encoder(config, self.num_encoders)\n",
        "        # Create a linear layer to project the encoder's output to the number of classes\n",
        "        self.classifier = nn.Linear(self.hidden_size, self.num_classes)\n",
        "        # Initialize the weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def forward(self, x,c, output_attentions=False):\n",
        "        # Calculate the embedding output\n",
        "\n",
        "        encoder_output, all_attentions = self.encoder(x, c,output_attentions=output_attentions)\n",
        "        # Calculate the logits, take the [CLS] token's output as features for classification\n",
        "        logits = self.classifier(encoder_output[:, 0, :])\n",
        "        # Return the logits and the attention probabilities (optional)\n",
        "        if not output_attentions:\n",
        "            return (logits, None)\n",
        "        else:\n",
        "            return (logits, all_attentions)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=self.config[\"initializer_range\"])\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "        elif isinstance(module, Embeddings):\n",
        "            module.position_embeddings.data = nn.init.trunc_normal_(\n",
        "                module.position_embeddings.data.to(torch.float32),\n",
        "                mean=0.0,\n",
        "                std=self.config[\"initializer_range\"],\n",
        "            ).to(module.position_embeddings.dtype)\n",
        "\n",
        "            module.cls_token.data = nn.init.trunc_normal_(\n",
        "                module.cls_token.data.to(torch.float32),\n",
        "                mean=0.0,\n",
        "                std=self.config[\"initializer_range\"],\n",
        "            ).to(module.cls_token.dtype)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbEp406Wqksp"
      },
      "outputs": [],
      "source": [
        "#@title Prepare Data 📊\n",
        "# Import libraries\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "def prepare_data(batch_size=4, num_workers=2, train_sample_size=None, test_sample_size=None):\n",
        "    train_transform = transforms.Compose(\n",
        "        [transforms.ToTensor(),\n",
        "        transforms.Resize((32, 32)),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomResizedCrop((32, 32), scale=(0.8, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=2),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    trainset = torchvision.datasets.CIFAR100(root='./data', train=True,\n",
        "                                            download=True, transform=train_transform)\n",
        "    if train_sample_size is not None:\n",
        "        # Randomly sample a subset of the training set\n",
        "        indices = torch.randperm(len(trainset))[:train_sample_size]\n",
        "        trainset = torch.utils.data.Subset(trainset, indices)\n",
        "\n",
        "\n",
        "\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                            shuffle=True, num_workers=num_workers)\n",
        "\n",
        "    test_transform = transforms.Compose(\n",
        "        [transforms.ToTensor(),\n",
        "        transforms.Resize((32, 32)),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    testset = torchvision.datasets.CIFAR100(root='./data', train=False,\n",
        "                                        download=True, transform=test_transform)\n",
        "    if test_sample_size is not None:\n",
        "        # Randomly sample a subset of the test set\n",
        "        indices = torch.randperm(len(testset))[:test_sample_size]\n",
        "        testset = torch.utils.data.Subset(testset, indices)\n",
        "\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                            shuffle=False, num_workers=num_workers)\n",
        "\n",
        "\n",
        "    return trainloader, testloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djY52Edqug3M"
      },
      "outputs": [],
      "source": [
        "#@title Utils 🛠️\n",
        "import json, os, math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "def save_experiment(experiment_name, config, train_losses, test_losses, accuracies, base_dir=\"experiments\"):\n",
        "    outdir = os.path.join(base_dir, experiment_name)\n",
        "    os.makedirs(outdir, exist_ok=True)\n",
        "\n",
        "    # Save the config\n",
        "    configfile = os.path.join(outdir, 'config.json')\n",
        "    with open(configfile, 'w') as f:\n",
        "        json.dump(config, f, sort_keys=True, indent=4)\n",
        "\n",
        "    # Save the metrics\n",
        "    jsonfile = os.path.join(outdir, 'metrics.json')\n",
        "    with open(jsonfile, 'w') as f:\n",
        "        data = {\n",
        "            'train_losses': train_losses,\n",
        "            'test_losses': test_losses,\n",
        "            'accuracies': accuracies,\n",
        "        }\n",
        "        json.dump(data, f, sort_keys=True, indent=4)\n",
        "\n",
        "\n",
        "\n",
        "def load_experiment(experiment_name,  base_dir=\"experiments\"):\n",
        "    outdir = os.path.join(base_dir, experiment_name)\n",
        "    # Load the config\n",
        "    configfile = os.path.join(outdir, 'config.json')\n",
        "    with open(configfile, 'r') as f:\n",
        "        config = json.load(f)\n",
        "    # Load the metrics\n",
        "    jsonfile = os.path.join(outdir, 'metrics.json')\n",
        "    with open(jsonfile, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    train_losses = data['train_losses']\n",
        "    test_losses = data['test_losses']\n",
        "    accuracies = data['accuracies']\n",
        "\n",
        "    return config, train_losses, test_losses, accuracies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGHq-Nl0rrFF",
        "outputId": "36807bbc-e4b4-4b5e-aab7-7f0d69313605"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch: 1, Train loss: 13.8004, Test loss: 7.7417, Accuracy: 0.0354\n"
          ]
        }
      ],
      "source": [
        "#@title Train ViT 🧠 🏋🏽\n",
        "#@title String fields\n",
        "\n",
        "exp_name = 'vit-with-cifar100' #@param {type:\"string\"}\n",
        "batch_size = 32 #@param {type: \"integer\"}\n",
        "epochs = 30 #@param {type: \"integer\"}\n",
        "lr = 5e-4  #@param {type: \"number\"}\n",
        "save_model_every = 0 #@param {type: \"integer\"}\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "config = {\n",
        "    \"patch_size\": 4,  # Input image size: 32x32 -> 8x8 patches\n",
        "    \"hidden_size\": 384,\n",
        "    \"num_hidden_layers\": 4,\n",
        "    \"num_attention_heads\": 4,\n",
        "    \"intermediate_size\": 4 * 384, # 4 * hidden_size\n",
        "    \"hidden_dropout_prob\": 0.0,\n",
        "    \"attention_probs_dropout_prob\": 0.0,\n",
        "    \"initializer_range\": 0.02,\n",
        "    \"image_size\": 32,\n",
        "    \"num_classes\": 100, # num_classes of CIFAR10\n",
        "    \"num_channels\": 3,\n",
        "    \"qkv_bias\": True,\n",
        "    \"use_faster_attention\": True,\n",
        "}\n",
        "# These are not hard constraints, but are used to prevent misconfigurations\n",
        "assert config[\"hidden_size\"] % config[\"num_attention_heads\"] == 0\n",
        "assert config['intermediate_size'] == 4 * config['hidden_size']\n",
        "assert config['image_size'] % config['patch_size'] == 0\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    \"\"\"\n",
        "    The simple trainer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_g, model_f, optimizer, loss_fn, exp_name, device):\n",
        "        self.model_g = model_g.to(device)\n",
        "        self.model_f = model_f.to(device)\n",
        "        self.optimizer = optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "        self.exp_name = exp_name\n",
        "        self.device = device\n",
        "        self.c = 0\n",
        "    def train(self, trainloader, testloader, epochs, save_model_every_n_epochs=0):\n",
        "        \"\"\"\n",
        "        Train the model for the specified number of epochs.\n",
        "        \"\"\"\n",
        "        # Keep track of the losses and accuracies\n",
        "        train_losses, test_losses, accuracies = [], [], []\n",
        "        # Train the model\n",
        "        for i in range(epochs):\n",
        "            train_loss = self.train_epoch(trainloader)\n",
        "            accuracy, test_loss = self.evaluate(testloader)\n",
        "            train_losses.append(train_loss)\n",
        "            test_losses.append(test_loss)\n",
        "            accuracies.append(accuracy)\n",
        "\n",
        "\n",
        "            print(f\"Epoch: {i+1}, Train loss: {train_loss:.4f}, Test loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "            # if save_model_every_n_epochs > 0 and (i+1) % save_model_every_n_epochs == 0 and i+1 != epochs:\n",
        "            #     print('\\tSave checkpoint at epoch', i+1)\n",
        "            #     save_checkpoint(self.exp_name, self.model, i+1)\n",
        "        # Save the experiment\n",
        "        save_experiment(self.exp_name, config, train_losses, test_losses, accuracies)\n",
        "\n",
        "    def train_epoch(self, trainloader):\n",
        "        \"\"\"\n",
        "        Train the model for one epoch.\n",
        "        \"\"\"\n",
        "        self.model_f.train()\n",
        "        self.model_g.train()\n",
        "\n",
        "        total_loss = 0\n",
        "        for batch in trainloader:\n",
        "            # Move the batch to the device\n",
        "            batch = [t.to(self.device) for t in batch]\n",
        "            images, labels = batch\n",
        "            # Zero the gradients\n",
        "            self.optimizer.zero_grad()\n",
        "            # Calculate the loss\n",
        "            y_temp = self.model_g(images,self.c)[0]\n",
        "            y_temp_copy = y_temp.detach().clone()\n",
        "            y_hat = self.model_f(y_temp_copy,self.c)[0]\n",
        "            loss = self.loss_fn(y_hat, labels)\n",
        "            # Backpropagate the loss\n",
        "            loss.backward()\n",
        "            # Update the model's parameters\n",
        "            self.optimizer.step()\n",
        "            total_loss += loss.item() * len(images)\n",
        "        return total_loss / len(trainloader.dataset)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def evaluate(self, testloader):\n",
        "        self.model_g.eval()\n",
        "        self.model_f.eval()\n",
        "\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in testloader:\n",
        "                # Move the batch to the device\n",
        "                batch = [t.to(self.device) for t in batch]\n",
        "                images, labels = batch\n",
        "\n",
        "                # Get predictions\n",
        "                y_temp = self.model_g(images,self.c)[0]\n",
        "                y_temp_copy = y_temp.detach().clone()\n",
        "                logits = self.model_f( y_temp_copy,self.c)[0]\n",
        "\n",
        "                # Calculate the loss\n",
        "                loss = self.loss_fn(logits, labels)\n",
        "                total_loss += loss.item() * len(images)\n",
        "\n",
        "                # Calculate the accuracy\n",
        "                predictions = torch.argmax(logits, dim=1)\n",
        "                correct += torch.sum(predictions == labels).item()\n",
        "        accuracy = correct / len(testloader.dataset)\n",
        "        avg_loss = total_loss / len(testloader.dataset)\n",
        "        return accuracy, avg_loss\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Training parameters\n",
        "    save_model_every_n_epochs = save_model_every\n",
        "    c = 0\n",
        "    # Load the CIFAR10 dataset\n",
        "    trainloader, testloader = prepare_data(batch_size=batch_size)\n",
        "    # Create the model, optimizer, loss function and trainer\n",
        "    model_f = ViTForClassfication_f(config)\n",
        "    model_g = ViTForClassfication_g(config)\n",
        "    optimizer = optim.Adam(list(model_g.parameters()) + list(model_f.parameters()), lr=5e-3, weight_decay=1e-6)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    trainer = Trainer(model_g,model_f, optimizer, loss_fn, exp_name, device=device)\n",
        "    trainer.train(trainloader, testloader, epochs, save_model_every_n_epochs=save_model_every_n_epochs)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTuHlgNgzfWW"
      },
      "outputs": [],
      "source": [
        "#@title Plot training Results\n",
        "config, train_losses, test_losses, accuracies = load_experiment(f\"/content/experiments/{exp_name}/\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# Create two subplots of train/test losses and accuracies\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "ax1.plot(train_losses, label=\"Train loss\")\n",
        "ax1.plot(test_losses, label=\"Test loss\")\n",
        "ax1.set_xlabel(\"Epoch\")\n",
        "ax1.set_ylabel(\"Loss\")\n",
        "ax1.legend()\n",
        "ax2.plot(accuracies)\n",
        "ax2.set_xlabel(\"Epoch\")\n",
        "ax2.set_ylabel(\"Accuracy\")\n",
        "plt.savefig(\"metrics.png\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}